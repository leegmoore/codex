===== PHASE 3: CODER PROMPT =====
ROLE: Senior TypeScript developer implementing phases of the Cody CLI integration project. You write clean, tested code following TDD principles with mocked-service tests at library boundaries.


---

PRODUCT:

**Cody** is a command-line interface for the Codex TypeScript library. It provides multi-provider LLM agent capabilities supporting OpenAI (Responses, Chat) and Anthropic (Messages) APIs with tool execution, conversation persistence, and structured tool calling. Built as a TypeScript port of OpenAI's Rust-based Codex CLI, Cody serves as both a standalone CLI tool and reference implementation for the @openai/codex-core library.


---

PROJECT CONTEXT:

# Project 02: UI Integration & Library Definition

## What We're Building

Project 02 integrates all ported Codex modules (Phases 1-6) into a working command-line interface called **Cody** and defines the library API surface for @openai/codex-core. This project validates the Rust → TypeScript port by wiring protocol, configuration, persistence, execution, client, tools, and orchestration layers into complete conversation flows.

## Why It Matters

The port is functionally complete but untested as an integrated system. Individual modules have unit tests, but we haven't verified end-to-end workflows. This project proves the port works, exposes integration issues, and establishes the library interface that external developers will use.

## Project Success Criteria

By project completion:
- User can create conversations, send messages, receive responses (all providers: OpenAI Responses/Chat, Anthropic Messages)
- All auth methods work (API keys, ChatGPT OAuth, Claude OAuth)
- Tools execute with approval flow
- Conversations persist and resume (JSONL format)
- MCP integration functional
- Library API documented (public exports, usage examples)
- REST API designed (optional implementation)
- Zero-error quality baseline maintained (0 TS errors, 0 ESLint errors, all tests passing)

## Dependencies

- Phase 6 complete (75 modules ported, 1,876 tests passing)
- Phase 5.2 complete (quality baseline clean)
- API keys: OpenAI, Anthropic, OpenRouter
- OAuth tokens: Read from ~/.codex (ChatGPT), ~/.claude (Claude)

## Scope

**In scope:** CLI commands, provider integration (3 APIs), auth methods (4 total), tool execution, persistence/resume, library API docs, REST API spec

**Non-scope:** Script harness (Project 03), memory innovations (Projects 04-06), rich TUI, additional tools, performance optimization, production hardening


---

PHASE 3 TECHNICAL DESIGN:

# Phase 3: Technical Design

**Phase:** Multi-Provider Support
**Goal:** Wire Chat Completions and Messages API alongside Responses, enable provider switching via CLI, verify parity across all three APIs

---

## Integration Overview

(From TECH-APPROACH Section 4)

Phase 3 adds Chat Completions and Messages API support alongside the Responses API from Phase 1. The provider abstraction from the port (WireApi enum, adapter pattern) means most heavy lifting is done—we're adding CLI commands for provider switching and verifying the three APIs work identically for end users. Same conversation code, different underlying API, transparent to CLI layer.

Provider switching happens via config or CLI command. User sets provider (openai or anthropic) and API type (responses, chat, or messages). ConversationManager constructs appropriate ModelClient based on config. From there, conversation flow is identical—sendMessage() works the same regardless of provider. The adapters (Phase 4.1-4.2) normalize provider-specific formats to common ResponseItems, making CLI code provider-agnostic.

Testing verifies parity: same conversation on all three providers produces equivalent results. Mock each provider's API responses, run identical conversation sequence, assert ResponseItems match expected structure. If provider-specific differences exist (thinking blocks in Messages, reasoning in Responses), document but don't block—test that they're handled gracefully.

### Phase 3 Target State

```
User runs: cody set-provider anthropic
           cody set-api messages

┌─────────────────────────────────┐
│  CLI (Phase 1-2 + NEW)          │
│  ┌──────────────────┐           │
│  │  set-provider   │ (NEW)      │
│  │  list-providers │ (NEW)      │
│  └────────┬─────────┘           │
│            ▼                     │
│    Provider Config Update        │
└────────────┬────────────────────┘
             ▼
┌──────────────────────────────────┐
│  ConversationManager             │
│  ┌────────────────────────────┐  │
│  │ ModelClient Factory (NEW) │  │
│  │  ┌─────────────────────┐  │  │
│  │  │ Switch on provider │  │  │
│  │  │ + API type         │  │  │
│  │  └─────────────────────┘  │  │
│  └────────────────────────────┘  │
└──────────┬───────────────────────┘
           ▼
    ┌──────┴──────┐
    ▼             ▼             ▼
┌────────┐  ┌──────────┐  ┌──────────┐
│Responses│  │   Chat   │  │ Messages │
│ Client │  │  Client  │  │  Client  │
│(Phase1)│  │  (NEW)   │  │  (NEW)   │
└────┬───┘  └────┬─────┘  └────┬─────┘
     │           │             │
     └───────────┴─────────────┘
                 ▼
          Common ResponseItems
                 ▼
            CLI Display
```

**Highlighted:** Provider switching commands (NEW), ModelClient factory (NEW), Chat and Messages clients (ACTIVATED from port), common ResponseItems abstraction (enables provider-agnostic CLI).

Each provider's client was ported in Phases 4.1-4.2 but never used in complete workflows. Phase 3 activates them, tests parity, and exposes any provider-specific quirks. The CLI doesn't know which provider is active—it just calls Conversation.sendMessage() and renders ResponseItems. Provider abstraction working as designed.

---

## Actual Signatures (from ported code)

### ModelClient Interface

Location: `codex-ts/src/core/client/`

The three client implementations all conform to a common interface, though the exact interface definition may vary. Each client's constructor and sendMessage method follow this pattern:

```typescript
// ResponsesClient (Phase 4.1)
class ResponsesClient {
  constructor(config: {
    apiKey: string;
    model: string;
    baseUrl?: string;
  })
  
  async sendMessage(request: ChatRequest): Promise<ResponseItems[]>
}

// ChatClient (Phase 4.1) 
class ChatClient {
  constructor(config: {
    apiKey: string;
    model: string;
    baseUrl?: string;
  })
  
  async sendMessage(request: ChatRequest): Promise<ResponseItems[]>
}

// MessagesClient (Phase 4.2)
class MessagesClient {
  constructor(config: {
    apiKey: string;
    model: string;
    baseUrl?: string;
  })
  
  async sendMessage(request: ChatRequest): Promise<ResponseItems[]>
}
```

**Key insight:** All three return `ResponseItems[]` (common format). The adapters handle provider-specific SSE parsing internally. From ConversationManager's perspective, they're interchangeable.

### WireApi Enum

Location: `codex-ts/src/core/model-provider-info.ts`

```typescript
export enum WireApi {
  Responses = 'responses',
  ChatCompletions = 'chat',
  Messages = 'messages'
}
```

This enum defines the three API types we support. Config uses string values ('responses', 'chat', 'messages'), factory maps to appropriate client.

### ConversationConfig Extension

```typescript
interface ConversationConfig {
  provider: 'openai' | 'anthropic';
  api: 'responses' | 'chat' | 'messages';
  model: string;
  auth: AuthConfig;
}
```

Phase 1 used implicit provider (always openai). Phase 3 makes it explicit and adds anthropic option.

---

## Technical Deltas

**New code (CLI layer):**
- src/cli/commands/set-provider.ts: Switch provider command (~40 lines)
- src/cli/commands/set-api.ts: Switch API type command (~30 lines)
- src/cli/commands/list-providers.ts: Show available providers (~40 lines)

**New code (integration):**
- src/core/conversation-manager.ts: Add createModelClient() factory method (~60 lines)
- Factory switches on provider + API type, constructs appropriate client

**New code (testing - mocked-service):**
- tests/mocked-service/phase-3-provider-parity.test.ts: Provider parity tests (~100 lines)
- tests/mocks/chat-client.ts: Mock ChatClient (~40 lines)
- tests/mocks/messages-client.ts: Mock MessagesClient (~40 lines)

**New code (testing - model integration):**
- scripts/integration-tests/phase-3/test-responses-api.ts (~50 lines)
- scripts/integration-tests/phase-3/test-chat-api.ts (~50 lines)
- scripts/integration-tests/phase-3/test-messages-api.ts (~50 lines)
- scripts/integration-tests/phase-3/test-openrouter.ts (~50 lines)
- scripts/integration-tests/phase-3/test-thinking-controls.ts (~60 lines)
- scripts/integration-tests/phase-3/test-temperature.ts (~60 lines)
- scripts/integration-tests/phase-3/run-all.ts (~40 lines)

**Estimated new code:** ~660 lines total
- CLI commands: ~110 lines
- Factory: ~60 lines
- Mocked-service tests: ~180 lines
- Model integration scripts: ~310 lines

---

## ModelClient Factory Implementation

**The factory pattern is central to Phase 3.** ConversationManager needs to construct the right client based on config without knowing implementation details.

```typescript
// In ConversationManager or separate factory module
private createModelClient(config: ConversationConfig, authManager: AuthManager): ModelClient {
  // Get API key for provider
  const apiKey = await authManager.getToken(config.provider);
  if (!apiKey) {
    throw new ConfigurationError(
      `Missing API key for provider ${config.provider}. ` +
      `Set via config or run: cody set-auth`
    );
  }

  // Construct client based on provider + API type
  const clientConfig = {
    apiKey,
    model: config.model,
    baseUrl: config.baseUrl // Optional override for testing
  };

  // OpenAI providers
  if (config.provider === 'openai') {
    if (config.api === 'responses') {
      return new ResponsesClient(clientConfig);
    }
    if (config.api === 'chat') {
      return new ChatClient(clientConfig);
    }
    throw new ConfigurationError(
      `Unsupported API type '${config.api}' for provider 'openai'. ` +
      `Supported: responses, chat`
    );
  }

  // Anthropic providers
  if (config.provider === 'anthropic') {
    if (config.api === 'messages') {
      return new MessagesClient(clientConfig);
    }
    throw new ConfigurationError(
      `Unsupported API type '${config.api}' for provider 'anthropic'. ` +
      `Supported: messages`
    );
  }

  throw new ConfigurationError(
    `Unknown provider '${config.provider}'. Supported: openai, anthropic`
  );
}
```

**Error messages are actionable:** Tell user what went wrong and what's supported. Factory throws before creating conversation, preventing invalid state.

**For testing:** Inject mock AuthManager that returns test tokens. Verify factory constructs correct client type (can spy on constructors or check client properties).

---

## CLI Command Implementation

### set-provider Command

```typescript
// src/cli/commands/set-provider.ts
import {program} from 'commander';
import {loadConfig, saveConfig} from '../config';

program
  .command('set-provider <provider>')
  .description('Set LLM provider (openai, anthropic)')
  .option('--api <api>', 'API type (responses, chat, messages)')
  .option('--model <model>', 'Model name')
  .action(async (provider: string, options) => {
    // Validate provider
    const validProviders = ['openai', 'anthropic'];
    if (!validProviders.includes(provider)) {
      console.error(`Invalid provider '${provider}'. Valid: ${validProviders.join(', ')}`);
      process.exit(1);
    }

    // Load current config
    const config = await loadConfig();
    
    // Update provider
    config.provider.name = provider;
    
    // Update API if provided
    if (options.api) {
      config.provider.api = options.api;
    } else {
      // Set default API for provider
      config.provider.api = provider === 'openai' ? 'responses' : 'messages';
    }
    
    // Update model if provided
    if (options.model) {
      config.provider.model = options.model;
    }
    
    // Save
    await saveConfig(config);
    
    console.log(`✓ Provider set to ${provider} (${config.provider.api})`);
    console.log(`  Model: ${config.provider.model}`);
  });
```

### list-providers Command

```typescript
// src/cli/commands/list-providers.ts
program
  .command('list-providers')
  .description('Show available providers and current selection')
  .action(async () => {
    const config = await loadConfig();
    const current = `${config.provider.name}/${config.provider.api}`;
    
    console.log('\nAvailable Providers:\n');
    
    const providers = [
      {name: 'openai', apis: ['responses', 'chat'], models: ['gpt-4', 'gpt-4o-mini']},
      {name: 'anthropic', apis: ['messages'], models: ['claude-3-haiku', 'claude-3-sonnet']},
    ];
    
    for (const p of providers) {
      for (const api of p.apis) {
        const combo = `${p.name}/${api}`;
        const marker = combo === current ? '→' : ' ';
        console.log(`${marker} ${combo}`);
        console.log(`    Models: ${p.models.join(', ')}`);
      }
    }
    
    console.log(`\nCurrent: ${current} (${config.provider.model})`);
  });
```

**UX considerations:** Current provider highlighted with arrow. Show example models for each provider/API combo. Clear visual hierarchy.

---

## Config File Updates

**Extended config format:**

```toml
[provider]
name = "openai"           # openai | anthropic
api = "responses"         # responses | chat | messages
model = "gpt-4o-mini"

[auth]
method = "api-key"
openai_api_key = "sk-..."
anthropic_api_key = "sk-ant-..."
```

**Config loader changes:**
- Read provider.name, provider.api, provider.model
- Validate combinations (anthropic only supports messages currently)
- Provide defaults if fields missing (openai/responses/gpt-4)

**Config writer changes:**
- Update provider fields
- Preserve other config sections (auth, tools, etc.)
- Atomic write (temp file + rename to avoid corruption)

---

## Component Structure

Provider abstraction uses factory pattern. ConversationManager checks config, constructs appropriate ModelClient. All clients implement common interface. CLI code is provider-agnostic.

```mermaid
classDiagram
    class CLI {
        +setProvider(name: string, api?: string)
        +listProviders()
    }

    class ConversationManager {
        -authManager: AuthManager
        +createConversation(config: ConversationConfig) Promise~Conversation~
        -createModelClient(config, auth) ModelClient
    }

    class ModelClient {
        <<interface>>
        +sendMessage(request: ChatRequest) Promise~ResponseItems[]~
    }

    class ResponsesClient {
        -apiKey: string
        -model: string
        +sendMessage(request) Promise~ResponseItems[]~
        -parseResponsesSSE(stream)
    }

    class ChatClient {
        -apiKey: string
        -model: string
        +sendMessage(request) Promise~ResponseItems[]~
        -parseChatSSE(stream)
        -aggregateDeltas()
    }

    class MessagesClient {
        -apiKey: string
        -model: string
        +sendMessage(request) Promise~ResponseItems[]~
        -parseMessagesSSE(stream)
        -adaptToResponseItems()
    }

    CLI --> ConversationManager: set provider
    ConversationManager --> ModelClient: creates via factory
    ModelClient <|.. ResponsesClient: implements
    ModelClient <|.. ChatClient: implements
    ModelClient <|.. MessagesClient: implements
```

---

## Connection Points Detail

### CLI → Config File

**set-provider command flow:**

User runs `cody set-provider anthropic --api messages --model claude-3-haiku`. Command handler loads current config from ~/.cody/config.toml. Updates provider.name, provider.api, provider.model fields. Validates combination (anthropic only supports messages). Saves updated config back to file. Prints confirmation with new settings.

**Validation rules:**
- Provider must be 'openai' or 'anthropic'
- API must be valid for provider (openai: responses/chat, anthropic: messages)
- Model should match provider (warn if mismatch, don't block)

**Error handling:**
- Invalid provider → list valid options, exit 1
- Unsupported API for provider → list valid APIs for that provider
- Config file write failure → print error with file path, suggest permissions check

### ConversationManager → ModelClient Factory

**Factory invocation:**

During createConversation(), ConversationManager calls internal createModelClient(config, authManager) method. Factory is private method (not exposed to CLI). Returns ModelClient interface. Manager stores client reference with conversation.

**Factory logic:**
1. Call authManager.getToken(config.provider) to get API key
2. If no token → throw ConfigurationError with helpful message
3. Switch on config.provider:
   - 'openai' → switch on config.api → construct ResponsesClient or ChatClient
   - 'anthropic' → verify config.api === 'messages' → construct MessagesClient
4. Return client (typed as ModelClient interface)

**Dependency injection:** AuthManager injected into ConversationManager constructor (Phase 1 pattern). Factory uses injected auth, doesn't create its own.

### ModelClient Implementations → ResponseItems

**The normalization layer:**

Each client adapter handles provider-specific SSE format and normalizes to ResponseItems array. This is where provider differences are absorbed.

**ResponsesClient (Phase 1):**
- Responses API already returns semantic events
- Events map directly to ResponseItems
- Minimal transformation needed
- Handles reasoning blocks (extended thinking)

**ChatClient (Phase 4.1):**
- Chat API returns deltas (token-by-token)
- ChatClient aggregates deltas into complete messages
- Converts aggregated messages to ResponseItems
- Handles function calls (tool requests)

**MessagesClient (Phase 4.2):**
- Messages API returns content blocks
- MessagesClient maps blocks to ResponseItems
- Handles thinking blocks (Claude's reasoning)
- Converts tool_use blocks to FunctionCall items

**All three produce compatible ResponseItems arrays.** CLI receives same structure regardless of provider. Display code doesn't know which provider was used.

### Provider-Specific Differences

**Thinking/Reasoning:**
- Responses API: `reasoning` content type
- Messages API: `thinking` content type
- Chat API: No native thinking (standard messages only)

**Tool calling:**
- All three support function calls
- Format differences handled by adapters
- CLI sees uniform FunctionCall items

**Streaming:**
- All three support SSE streaming
- Chunk formats differ (handled by client parsers)
- CLI receives complete ResponseItems after aggregation

**These differences are transparent to CLI.** Adapters normalize everything.

---

## Error Handling

**Configuration errors:**
```typescript
// Missing API key
throw new ConfigurationError(
  `Missing API key for provider ${provider}. ` +
  `Set in config: [auth]\n${provider}_api_key = "..."`
);

// Unsupported combination
throw new ConfigurationError(
  `Provider '${provider}' does not support API '${api}'. ` +
  `Supported APIs for ${provider}: ${getSupportedApis(provider).join(', ')}`
);

// Unknown provider
throw new ConfigurationError(
  `Unknown provider '${provider}'. ` +
  `Supported providers: openai, anthropic`
);
```

**Runtime errors:**
- Network failures → bubble from ModelClient (Phase 1 already handles)
- Auth failures → bubble from ModelClient (401/403 errors)
- Rate limits → bubble from ModelClient (429 errors)

**CLI error display:**
- Catch ConfigurationError → print message, suggest fix, exit 1
- Catch NetworkError → print "API call failed", suggest retry
- Catch AuthError → print "Authentication failed", suggest checking keys

---

## Mock Implementation Guide

### Mock Provider Clients

For testing, create mocks that return preset ResponseItems without making API calls.

```typescript
// tests/mocks/provider-clients.ts

export function createMockResponsesClient(responses: ResponseItems[][] = []): ResponsesClient {
  let callIndex = 0;
  return {
    sendMessage: vi.fn().mockImplementation(async (req: ChatRequest) => {
      const response = responses[callIndex++] || [
        {type: 'message', role: 'assistant', content: [{type: 'text', text: 'Mock response'}]}
      ];
      return response;
    })
  } as unknown as ResponsesClient;
}

export function createMockChatClient(responses: ResponseItems[][] = []): ChatClient {
  // Same pattern as ResponsesClient
  let callIndex = 0;
  return {
    sendMessage: vi.fn().mockImplementation(async (req: ChatRequest) => {
      return responses[callIndex++] || defaultResponse;
    })
  } as unknown as ChatClient;
}

export function createMockMessagesClient(responses: ResponseItems[][] = []): MessagesClient {
  // Same pattern
  let callIndex = 0;
  return {
    sendMessage: vi.fn().mockImplementation(async (req: ChatRequest) => {
      return responses[callIndex++] || defaultResponse;
    })
  } as unknown as MessagesClient;
}
```

**Usage in tests:**
```typescript
const mockResponses = createMockResponsesClient([
  [{type: 'message', role: 'assistant', content: [{type: 'text', text: 'Hello from Responses'}]}]
]);

const mockChat = createMockChatClient([
  [{type: 'message', role: 'assistant', content: [{type: 'text', text: 'Hello from Chat'}]}]
]);

const mockMessages = createMockMessagesClient([
  [{type: 'message', role: 'assistant', content: [{type: 'text', text: 'Hello from Messages'}]}]
]);
```

### Mock AuthManager

```typescript
// tests/mocks/auth-manager.ts

export function createMockAuth(tokens: Record<string, string>): AuthManager {
  return {
    async getToken(provider: string): Promise<string> {
      const token = tokens[provider];
      if (!token) {
        throw new Error(`No token configured for provider ${provider}`);
      }
      return token;
    }
  } as unknown as AuthManager;
}

// Usage
const mockAuth = createMockAuth({
  'openai': 'test-openai-key',
  'anthropic': 'test-anthropic-key'
});
```

**For negative tests:** Return null or throw to test error paths.

---

## Model Integration Testing

**Purpose:** Validate real provider behavior with actual API calls using cheap models.

Phase 3 is the first time we test with real LLM providers. Mocked-service tests verify wiring correctness, but model integration tests verify providers actually work. These tests cost money (pennies with cheap models) and require API keys, so they're separate from main test suite.

### Integration Test Scripts

Located in `scripts/integration-tests/phase-3/`, each script tests one provider with real API:

**test-responses-api.ts:**
```typescript
// Test OpenAI Responses API with gpt-4o-mini
import {ConversationManager} from '../../src/core/conversation-manager';
import {AuthManager} from '../../src/core/auth';

async function testResponsesAPI() {
  const auth = new AuthManager({
    method: 'api-key',
    openai_api_key: process.env.OPENAI_API_KEY
  });
  
  const manager = new ConversationManager(auth, null);
  const config = {
    provider: 'openai',
    api: 'responses',
    model: 'gpt-4o-mini',
    auth: {method: 'api-key'}
  };
  
  console.log('Testing OpenAI Responses API...');
  const {conversation} = await manager.newConversation(config);
  
  // Send test message
  await conversation.submit([{type: 'text', text: 'Say hello in one sentence'}]);
  const event = await conversation.nextEvent();
  
  console.log('✓ Response received:', event.msg);
  console.log('✓ Responses API working\n');
}

testResponsesAPI().catch(console.error);
```

**Similar scripts for:**
- test-chat-api.ts (OpenAI Chat with gpt-4o-mini)
- test-messages-api.ts (Anthropic Messages with haiku-4.5)
- test-openrouter.ts (OpenRouter with gemini-2.0-flash-001)

### Config Parameter Tests

**test-thinking-controls.ts:**
```typescript
// Test thinking mode with Responses and Messages APIs
async function testThinking() {
  // Test 1: Responses API with thinking enabled
  const responsesConfig = {
    provider: 'openai',
    api: 'responses',
    model: 'gpt-4o-mini',
    thinking: {mode: 'enabled', budget: 5000}
  };
  // Send message, verify reasoning blocks in response
  
  // Test 2: Messages API with thinking enabled
  const messagesConfig = {
    provider: 'anthropic',
    api: 'messages',
    model: 'claude-3-haiku',
    thinking: {mode: 'enabled', budget: 5000}
  };
  // Send message, verify thinking blocks in response
  
  console.log('✓ Thinking mode works on both providers');
}
```

**test-temperature.ts:**
```typescript
// Test temperature variation produces different outputs
async function testTemperature() {
  const baseConfig = {provider: 'openai', api: 'chat', model: 'gpt-4o-mini'};
  
  // Same prompt, different temperatures
  const temps = [0.2, 0.7, 1.0];
  const responses = [];
  
  for (const temp of temps) {
    const config = {...baseConfig, temperature: temp};
    const {conversation} = await manager.newConversation(config);
    await conversation.submit([{type: 'text', text: 'Write a creative sentence about coding'}]);
    const event = await conversation.nextEvent();
    responses.push(event.msg);
  }
  
  // Verify responses differ (higher temp = more variation)
  console.log('Temperature 0.2:', responses[0]);
  console.log('Temperature 0.7:', responses[1]);
  console.log('Temperature 1.0:', responses[2]);
  console.log('✓ Temperature controls working');
}
```

### Run All Script

```typescript
// scripts/integration-tests/phase-3/run-all.ts
import {testResponsesAPI} from './test-responses-api';
import {testChatAPI} from './test-chat-api';
import {testMessagesAPI} from './test-messages-api';
import {testOpenRouter} from './test-openrouter';
import {testThinking} from './test-thinking-controls';
import {testTemperature} from './test-temperature';

async function runAll() {
  console.log('=== Phase 3 Model Integration Tests ===\n');
  
  const tests = [
    {name: 'Responses API', fn: testResponsesAPI},
    {name: 'Chat API', fn: testChatAPI},
    {name: 'Messages API', fn: testMessagesAPI},
    {name: 'OpenRouter', fn: testOpenRouter},
    {name: 'Thinking Controls', fn: testThinking},
    {name: 'Temperature', fn: testTemperature}
  ];
  
  const results = [];
  
  for (const test of tests) {
    try {
      await test.fn();
      results.push({name: test.name, status: 'PASS'});
    } catch (err) {
      results.push({name: test.name, status: 'FAIL', error: err.message});
    }
  }
  
  console.log('\n=== Results ===');
  for (const r of results) {
    const icon = r.status === 'PASS' ? '✓' : '✗';
    console.log(`${icon} ${r.name}: ${r.status}`);
    if (r.error) console.log(`  Error: ${r.error}`);
  }
  
  const passed = results.filter(r => r.status === 'PASS').length;
  console.log(`\n${passed}/${results.length} tests passed`);
  
  process.exit(passed === results.length ? 0 : 1);
}

runAll();
```

**Run via:** `npm run test:integration` (add to package.json scripts)

**Requirements:**
- API keys in .env (OPENAI_API_KEY, ANTHROPIC_API_KEY, OPENROUTER_API_KEY)
- Network access
- ~30 seconds runtime
- ~$0.01-0.05 cost (cheap models, short prompts)

---

## Reference Locations

**Ported client implementations:**
- ResponsesClient: `codex-ts/src/core/client/responses/client.ts`
- ChatClient: `codex-ts/src/core/client/chat/client.ts` (or chat-completions/)
- MessagesClient: `codex-ts/src/core/client/messages/adapter.ts`

**Config system:**
- Config types: `codex-ts/src/core/config/types.ts`
- Config loader: `codex-ts/src/core/config-loader/index.ts`

**ConversationManager:**
- Main file: `codex-ts/src/core/conversation-manager/index.ts`

**CLI commands:**
- Existing commands: `codex-ts/src/cli/commands/` (new, chat from Phase 1)
- Add provider commands here

---

## Implementation Notes

**Key decisions to document in DECISIONS.md:**

1. **Default API per provider:** When user sets provider without specifying API, use defaults:
   - openai → responses (most stable)
   - anthropic → messages (only option)

2. **Factory location:** Put createModelClient() as private method in ConversationManager vs separate factory module. Recommend: private method (simpler, fewer files).

3. **Client reuse:** Should ConversationManager reuse client instances across conversations (same provider/model) or create fresh per conversation? Recommend: fresh per conversation (simpler state management, minimal overhead).

4. **Config validation:** Validate provider/API combination at config save time (set-provider command) or at conversation creation time? Recommend: both (early feedback at save, safety check at creation).

5. **Model name validation:** Should we validate model names against known models or allow any string? Recommend: allow any (new models released frequently, don't want to block).

**Error message tone:** Helpful and actionable. Don't just say "invalid config"—say "Provider 'anthropic' only supports API 'messages'. You specified 'chat'."

**CLI UX:** Show current provider prominently. Use visual markers (arrows, colors) to highlight selection in list-providers output.

---

## Verification Approach

### Mocked-Service Testing (Automated)

Tests in `tests/mocked-service/phase-3-provider-parity.test.ts` verify wiring correctness with mocked clients.

```typescript
describe('Phase 3: Multi-Provider Support', () => {
  let mockAuth: AuthManager;
  let manager: ConversationManager;

  beforeEach(() => {
    mockAuth = createMockAuth({
      'openai': 'test-openai-key',
      'anthropic': 'test-anthropic-key'
    });
    manager = new ConversationManager(mockAuth, null);
  });

  describe('Provider Parity', () => {
    it('Responses API works', async () => {
      const mockClient = createMockResponsesClient([
        [{type: 'message', role: 'assistant', content: [{type: 'text', text: 'Hello'}]}]
      ]);
      
      // Inject mock client (or spy on factory)
      const config = {provider: 'openai', api: 'responses', model: 'gpt-4', auth: {method: 'api-key'}};
      const {conversation} = await manager.newConversation(config);
      
      await conversation.submit([{type: 'text', text: 'Hi'}]);
      const event = await conversation.nextEvent();
      
      expect(event.msg).toBeDefined();
      // Verify ResponsesClient was used (spy on constructor or check client type)
    });

    it('Chat API works', async () => {
      const mockClient = createMockChatClient([
        [{type: 'message', role: 'assistant', content: [{type: 'text', text: 'Hello'}]}]
      ]);
      
      const config = {provider: 'openai', api: 'chat', model: 'gpt-4', auth: {method: 'api-key'}};
      const {conversation} = await manager.newConversation(config);
      
      await conversation.submit([{type: 'text', text: 'Hi'}]);
      const event = await conversation.nextEvent();
      
      expect(event.msg).toBeDefined();
    });

    it('Messages API works', async () => {
      const mockClient = createMockMessagesClient([
        [{type: 'message', role: 'assistant', content: [{type: 'text', text: 'Hello'}]}]
      ]);
      
      const config = {provider: 'anthropic', api: 'messages', model: 'claude-3-haiku', auth: {method: 'api-key'}};
      const {conversation} = await manager.newConversation(config);
      
      await conversation.submit([{type: 'text', text: 'Hi'}]);
      const event = await conversation.nextEvent();
      
      expect(event.msg).toBeDefined();
    });

    it('all providers return compatible ResponseItems', async () => {
      // Run same conversation on all three, compare structure
      const configs = [
        {provider: 'openai', api: 'responses'},
        {provider: 'openai', api: 'chat'},
        {provider: 'anthropic', api: 'messages'}
      ];
      
      for (const cfg of configs) {
        const {conversation} = await manager.newConversation({...cfg, model: 'test', auth: {method: 'api-key'}});
        await conversation.submit([{type: 'text', text: 'Test'}]);
        const event = await conversation.nextEvent();
        
        // Verify structure (should be consistent)
        expect(event.msg).toHaveProperty('items');
        expect(Array.isArray(event.msg.items)).toBe(true);
      }
    });
  });

  describe('Configuration', () => {
    it('set-provider updates config', async () => {
      // Test CLI command updates config file
      // Use temp config for test isolation
    });

    it('unsupported combination throws', async () => {
      const config = {provider: 'anthropic', api: 'chat', model: 'claude', auth: {method: 'api-key'}};
      await expect(manager.newConversation(config)).rejects.toThrow('Unsupported');
    });

    it('missing API key throws', async () => {
      const mockAuth = createMockAuth({}); // No tokens
      const manager = new ConversationManager(mockAuth, null);
      
      const config = {provider: 'openai', api: 'responses', model: 'gpt-4', auth: {method: 'api-key'}};
      await expect(manager.newConversation(config)).rejects.toThrow('Missing API key');
    });
  });

  describe('CLI Commands', () => {
    it('list-providers shows all options', async () => {
      // Execute command, capture stdout, verify output
    });
  });
});
```

### Model Integration Testing (Manual)

**Run after mocked-service tests pass.** These make real API calls to validate provider behavior.

**Execution:**
```bash
# Ensure API keys set
export OPENAI_API_KEY=sk-...
export ANTHROPIC_API_KEY=sk-ant-...
export OPENROUTER_API_KEY=sk-or-...

# Run integration suite
npm run test:integration

# Or run individual tests
node scripts/integration-tests/phase-3/test-responses-api.ts
node scripts/integration-tests/phase-3/test-messages-api.ts
```

**What we're validating:**
- Real API calls succeed (network, auth, endpoints work)
- Responses are coherent (models actually respond)
- Config parameters work (thinking, temperature affect output)
- Provider-specific features work (reasoning blocks, thinking blocks)
- Error handling works (rate limits, auth failures surface correctly)

**Expected results:**
- All 6 scripts pass (print success messages)
- Total runtime: ~30 seconds
- Total cost: ~$0.01-0.05
- Any failures → investigate provider-specific issues

**Log results:** Note any provider quirks, edge cases, or unexpected behavior in DECISIONS.md. These inform error handling and documentation.

---

## Quality Gates

**Before marking Phase 3 complete:**

1. **Mocked-service tests:** `npm test tests/mocked-service/phase-3-provider-parity.test.ts` → all passing
2. **Unit tests:** Baseline maintained (1,876+ still passing)
3. **Model integration:** `npm run test:integration` → all 6 scripts pass
4. **TypeScript:** `npx tsc --noEmit` → 0 errors
5. **ESLint:** `npm run lint` → 0 errors
6. **Format:** `npm run format` → no changes
7. **Combined:** All checks in sequence, no failures

**Manual verification:** Follow manual-test-script.md (5 scenarios), verify all work.

**Code review:**
- Stage 1 (Traditional): Provider switching logic, config handling, error messages, CLI UX
- Stage 2 (Port Validation): Provider abstraction preserved, adapter patterns correct, model integration results reviewed

---

## Summary

Phase 3 activates the multi-provider abstraction from the port. The heavy lifting (client adapters, SSE parsing, normalization) was done in Phases 4.1-4.2. This phase adds the factory pattern, CLI commands for switching, and comprehensive testing (mocked + real APIs) to verify parity. Focus on clean factory abstraction, helpful CLI UX, and thorough testing across all three providers with both mocked and real models.


---

TEST CONDITIONS:

# Phase 3: Test Conditions

**Test Frameworks:** 
- Vitest (mocked-service tests)
- Node.js scripts (model integration tests)

---

## Mocked-Service Tests

**File:** `tests/mocked-service/phase-3-provider-parity.test.ts`
**Mocks:** `tests/mocks/provider-clients.ts`, `tests/mocks/auth-manager.ts`

### Suite 1: Provider Parity

**Test 1: Responses API Works**
- **Setup:** Mock ResponsesClient returning assistant message, config provider=openai api=responses
- **Execute:** Create conversation, send message
- **Verify:** ResponseItems returned, ModelClient.sendMessage called, response structure valid

**Test 2: Chat API Works**
- **Setup:** Mock ChatClient, config provider=openai api=chat
- **Execute:** Create conversation, send message
- **Verify:** ChatClient used (spy on constructor or factory), response valid

**Test 3: Messages API Works**
- **Setup:** Mock MessagesClient, config provider=anthropic api=messages
- **Execute:** Create conversation, send message
- **Verify:** MessagesClient used, response valid

**Test 4: All Providers Return Compatible ResponseItems**
- **Setup:** Run same conversation on all three providers with mocked clients
- **Execute:** Send identical message to each
- **Verify:** Response structure consistent (all have type='message', role='assistant', content array)

### Suite 2: Configuration & Validation

**Test 5: Provider Switching Persists**
- **Setup:** Start with openai/responses
- **Execute:** Run CLI command `cody set-provider anthropic --api messages`, reload config
- **Verify:** Config updated, new conversation uses MessagesClient

**Test 6: Missing API Key Throws**
- **Setup:** AuthManager mock returns null for requested provider
- **Execute:** Attempt createConversation()
- **Verify:** Throws ConfigurationError with message "Missing API key for provider X"

**Test 7: Unsupported Combination Error**
- **Setup:** Config provider=anthropic api=chat (invalid combo)
- **Execute:** Attempt createConversation()
- **Verify:** Throws ConfigurationError with message "Unsupported provider/api combination"

**Test 8: Unknown Provider Error**
- **Setup:** Config provider=invalid
- **Execute:** Attempt createConversation()
- **Verify:** Throws ConfigurationError listing valid providers

### Suite 3: CLI Commands

**Test 9: list-providers Output**
- **Execute:** Run `cody list-providers`
- **Verify:** Output includes all valid combinations, current provider highlighted with arrow

**Test 10: set-provider Validation**
- **Execute:** Run `cody set-provider invalid-name`
- **Verify:** Error message, exit code 1, lists valid providers

**Test 11: set-api Validation**
- **Setup:** Current provider=anthropic
- **Execute:** Run `cody set-api chat`
- **Verify:** Error message "anthropic does not support chat", suggests valid APIs

---

## Model Integration Tests

**Location:** `scripts/integration-tests/phase-3/`
**Run via:** `npm run test:integration` or `node scripts/integration-tests/phase-3/run-all.ts`
**Requirements:** Real API keys in .env, network access

### Script 1: test-responses-api.ts

**Test:** OpenAI Responses API with gpt-4o-mini
- **Setup:** Real AuthManager with OPENAI_API_KEY, config api=responses
- **Execute:** Create conversation, send "Say hello in one sentence"
- **Verify:** Response received, no errors, response is coherent text
- **Log:** Response content, latency, token usage

### Script 2: test-chat-api.ts

**Test:** OpenAI Chat Completions with gpt-4o-mini
- **Setup:** Real AuthManager, config api=chat
- **Execute:** Create conversation, send "Say hello in one sentence"
- **Verify:** Response received, no errors, response is coherent
- **Log:** Response content, latency, token usage

### Script 3: test-messages-api.ts

**Test:** Anthropic Messages API with haiku-4.5
- **Setup:** Real AuthManager with ANTHROPIC_API_KEY, config api=messages
- **Execute:** Create conversation, send "Say hello in one sentence"
- **Verify:** Response received, no errors, response is coherent
- **Log:** Response content, latency, token usage

### Script 4: test-openrouter.ts

**Test:** OpenRouter with gemini-2.0-flash-001
- **Setup:** Real AuthManager with OPENROUTER_API_KEY, config provider=openai api=chat model=google/gemini-2.0-flash-001
- **Execute:** Create conversation, send "Say hello in one sentence"
- **Verify:** Response received via OpenRouter, Gemini model responds
- **Log:** Response content, latency, token usage

### Script 5: test-thinking-controls.ts

**Test:** Thinking mode with Responses and Messages APIs
- **Test 5a:** OpenAI Responses with thinking enabled
  - Config: api=responses, model=gpt-4o-mini, thinking={mode: 'enabled', budget: 5000}
  - Send: "Explain why the sky is blue"
  - Verify: Response includes reasoning blocks (check for reasoning content type)
  
- **Test 5b:** Anthropic Messages with thinking enabled
  - Config: api=messages, model=claude-3-haiku, thinking={mode: 'enabled', budget: 5000}
  - Send: "Explain why the sky is blue"
  - Verify: Response includes thinking blocks (check for thinking content type)

**Log:** Whether thinking blocks present, token usage difference

### Script 6: test-temperature.ts

**Test:** Temperature variation produces different outputs
- **Setup:** Same prompt, three temperatures (0.2, 0.7, 1.0)
- **Execute:** Send "Write a creative sentence about coding" with each temperature
- **Verify:** Responses differ (higher temp = more variation), all coherent
- **Log:** All three responses, compare creativity/variation

### Script 7: run-all.ts

**Test:** Execute all 6 scripts, collect results
- **Execute:** Run each script in sequence
- **Verify:** All pass (exit code 0)
- **Log:** Summary table (test name, status, errors if any)

**Expected runtime:** ~30 seconds total
**Expected cost:** ~$0.01-0.05 (cheap models, short prompts)

---

## Mock Strategy

**For mocked-service tests:**

**Mock provider clients:**
- createMockResponsesClient(responses: ResponseItems[][])
- createMockChatClient(responses: ResponseItems[][])
- createMockMessagesClient(responses: ResponseItems[][])
- Each returns mock with sendMessage spy

**Mock AuthManager:**
- createMockAuth(tokens: Record<string, string>)
- Returns token per provider
- Can configure to return null for negative tests

**Mock config file:**
- Use temp directory for CLI command tests
- Avoid touching real ~/.cody/config.toml during tests

**For model integration scripts:**
- No mocks (real API calls)
- Use real AuthManager, real clients
- Require API keys in environment

---

## Verification Checklist

**Automated tests pass:**
- [ ] Mocked-service tests: 11 tests in phase-3-provider-parity.test.ts
- [ ] All tests pass in <2 seconds
- [ ] No skipped tests

**Model integration pass:**
- [ ] test-responses-api.ts: ✓
- [ ] test-chat-api.ts: ✓
- [ ] test-messages-api.ts: ✓
- [ ] test-openrouter.ts: ✓
- [ ] test-thinking-controls.ts: ✓
- [ ] test-temperature.ts: ✓
- [ ] run-all.ts: 6/6 passing

**Quality gates:**
- [ ] TypeScript: 0 errors
- [ ] ESLint: 0 errors
- [ ] Format: no changes
- [ ] Combined: `npm run format && npm run lint && npx tsc --noEmit && npm test` succeeds

**Manual verification:**
- [ ] Follow manual-test-script.md (5 scenarios)
- [ ] All providers work in actual CLI usage

**Code review:**
- [ ] Stage 1 (Traditional): Provider logic, config handling, errors, UX
- [ ] Stage 2 (Port Validation): Abstraction preserved, adapters correct, integration results reviewed

---

**All checks ✓ → Phase 3 complete**


---

TASKS (update source/checklist.md as you work):

# Phase 3: Multi-Provider Support – Task Checklist

**Status:** Not Started
**Estimated Code:** ~660 lines (CLI 110, factory 60, mocked tests 180, integration scripts 310)

---

## Setup & Planning

- [ ] Review Phase 2 implementation (ensure tool execution works)
- [ ] Read TECH-APPROACH Section 4 (Phase 3 technical approach)
- [ ] Read phase-3/source/design.md (implementation details)
- [ ] Inspect ported client implementations:
  - [ ] ResponsesClient: `codex-ts/src/core/client/responses/client.ts`
  - [ ] ChatClient: `codex-ts/src/core/client/chat/` or `chat-completions/`
  - [ ] MessagesClient: `codex-ts/src/core/client/messages/adapter.ts`

---

## Core: ModelClient Factory

- [ ] Create factory method in ConversationManager (or separate module)
- [ ] Implement provider switching logic:
  - [ ] Switch on config.provider ('openai' | 'anthropic')
  - [ ] Switch on config.api ('responses' | 'chat' | 'messages')
  - [ ] Construct appropriate client with apiKey + model
- [ ] Error handling:
  - [ ] Missing API key → ConfigurationError with helpful message
  - [ ] Unsupported combination → ConfigurationError listing valid options
  - [ ] Unknown provider → ConfigurationError listing valid providers
- [ ] Verify factory returns ModelClient interface (type-safe)
- [ ] Document factory location decision in DECISIONS.md

---

## Config System Updates

- [ ] Extend ConversationConfig type:
  - [ ] Add provider: 'openai' | 'anthropic'
  - [ ] Add api: 'responses' | 'chat' | 'messages'
  - [ ] Keep model: string
- [ ] Update config loader to read provider fields from TOML
- [ ] Update config writer to save provider fields
- [ ] Add validation:
  - [ ] Validate provider/API combination at save time
  - [ ] Validate again at conversation creation (safety check)
- [ ] Set defaults when fields missing:
  - [ ] Default provider: openai
  - [ ] Default API: responses (for openai), messages (for anthropic)
  - [ ] Default model: gpt-4 (for openai), claude-3-haiku (for anthropic)
- [ ] Document defaults in DECISIONS.md

---

## CLI Commands

### set-provider Command

- [ ] Create `src/cli/commands/set-provider.ts`
- [ ] Implement command:
  - [ ] Accept provider argument (openai, anthropic)
  - [ ] Accept --api flag (optional)
  - [ ] Accept --model flag (optional)
- [ ] Validation:
  - [ ] Check provider is valid
  - [ ] Check API is valid for provider
  - [ ] Helpful error messages for invalid inputs
- [ ] Config update:
  - [ ] Load current config
  - [ ] Update provider fields
  - [ ] Save to file
- [ ] Output:
  - [ ] Print confirmation: "✓ Provider set to X (Y)"
  - [ ] Show model being used

### set-api Command

- [ ] Create `src/cli/commands/set-api.ts`
- [ ] Implement command:
  - [ ] Accept api argument (responses, chat, messages)
  - [ ] Update current provider's API type
- [ ] Validation:
  - [ ] Check API valid for current provider
  - [ ] Error if unsupported combination
- [ ] Save and confirm

### list-providers Command

- [ ] Create `src/cli/commands/list-providers.ts`
- [ ] Implement command:
  - [ ] Show all valid provider/API combinations
  - [ ] Highlight current selection with arrow (→)
  - [ ] Show example models for each combo
- [ ] Format output:
  - [ ] Clear visual hierarchy
  - [ ] Current selection obvious
  - [ ] Helpful for user deciding what to switch to

---

## Mocked-Service Tests (TDD)

- [ ] Create `tests/mocked-service/phase-3-provider-parity.test.ts`
- [ ] Create mock implementations:
  - [ ] tests/mocks/provider-clients.ts (createMockResponsesClient, createMockChatClient, createMockMessagesClient)
  - [ ] tests/mocks/auth-manager.ts (createMockAuth)

### Provider Parity Suite

- [ ] Test 1: Responses API works (mock ResponsesClient, verify conversation flow)
- [ ] Test 2: Chat API works (mock ChatClient, verify conversation flow)
- [ ] Test 3: Messages API works (mock MessagesClient, verify conversation flow)
- [ ] Test 4: All providers return compatible ResponseItems (structure parity check)

### Configuration Suite

- [ ] Test 5: Provider switching persists (CLI command updates config, new conversation uses new provider)
- [ ] Test 6: Missing API key throws (AuthManager returns null, expect ConfigurationError)
- [ ] Test 7: Unsupported combination error (anthropic + chat → error)
- [ ] Test 8: Unknown provider error (invalid provider name → error)

### CLI Commands Suite

- [ ] Test 9: list-providers shows all options (capture stdout, verify output)
- [ ] Test 10: set-provider validation (invalid provider → error message)
- [ ] Test 11: set-api validation (unsupported API for provider → error message)

- [ ] All 11 tests passing
- [ ] Tests run fast (<2 seconds)
- [ ] No skipped tests

---

## Model Integration Scripts

**Location:** `scripts/integration-tests/phase-3/`
**Purpose:** Validate real provider behavior with actual API calls

### Script Setup

- [ ] Create directory: `scripts/integration-tests/phase-3/`
- [ ] Ensure .env has API keys:
  - [ ] OPENAI_API_KEY
  - [ ] ANTHROPIC_API_KEY
  - [ ] OPENROUTER_API_KEY
- [ ] Add npm script: `"test:integration": "node scripts/integration-tests/phase-3/run-all.ts"`

### Individual Scripts

- [ ] **test-responses-api.ts** (OpenAI Responses, gpt-4o-mini)
  - [ ] Create conversation with real ResponsesClient
  - [ ] Send test message: "Say hello in one sentence"
  - [ ] Verify response received, log content and latency
  - [ ] Exit 0 on success, 1 on failure

- [ ] **test-chat-api.ts** (OpenAI Chat, gpt-4o-mini)
  - [ ] Create conversation with real ChatClient
  - [ ] Send test message
  - [ ] Verify response, log results

- [ ] **test-messages-api.ts** (Anthropic Messages, haiku-4.5)
  - [ ] Create conversation with real MessagesClient
  - [ ] Send test message
  - [ ] Verify response, log results

- [ ] **test-openrouter.ts** (OpenRouter, gemini-2.0-flash-001)
  - [ ] Create conversation via OpenRouter
  - [ ] Model: google/gemini-2.0-flash-001
  - [ ] Send test message
  - [ ] Verify Gemini responds via OpenRouter

- [ ] **test-thinking-controls.ts** (Thinking mode)
  - [ ] Test 5a: Responses API with thinking enabled
    - [ ] Config: thinking={mode: 'enabled', budget: 5000}
    - [ ] Send: "Explain why the sky is blue"
    - [ ] Verify: Response includes reasoning blocks
  - [ ] Test 5b: Messages API with thinking enabled
    - [ ] Config: thinking={mode: 'enabled', budget: 5000}
    - [ ] Send: "Explain why the sky is blue"
    - [ ] Verify: Response includes thinking blocks
  - [ ] Log: Token usage with/without thinking

- [ ] **test-temperature.ts** (Temperature variation)
  - [ ] Send same prompt with temp=0.2, 0.7, 1.0
  - [ ] Prompt: "Write a creative sentence about coding"
  - [ ] Verify: Responses differ (higher temp = more variation)
  - [ ] Log: All three responses for comparison

- [ ] **run-all.ts** (Execute suite)
  - [ ] Import all 6 test scripts
  - [ ] Run in sequence
  - [ ] Catch errors, collect results
  - [ ] Print summary table (test name, status, errors)
  - [ ] Exit 0 if all pass, 1 if any fail

### Integration Test Execution

- [ ] Run: `npm run test:integration`
- [ ] Verify: All 6 scripts pass
- [ ] Review: Any provider-specific quirks or issues
- [ ] Document: Note findings in DECISIONS.md
- [ ] Cost check: Should be ~$0.01-0.05 total

---

## Quality Gates

### Code Quality

- [ ] Run: `npm run format`
- [ ] Verify: No file changes (already formatted)
- [ ] Run: `npm run lint`
- [ ] Verify: 0 errors (warnings acceptable)
- [ ] Run: `npx tsc --noEmit`
- [ ] Verify: 0 TypeScript errors

### Testing

- [ ] Run: `npm test`
- [ ] Verify: All tests passing (1,876+ baseline + 11 new mocked-service tests)
- [ ] Verify: 0 skipped tests
- [ ] Run: `npm run test:integration`
- [ ] Verify: All 6 integration scripts pass
- [ ] Review: Integration test logs for issues

### Combined Verification

- [ ] Run: `npm run format && npm run lint && npx tsc --noEmit && npm test`
- [ ] Verify: All commands succeed in sequence
- [ ] Screenshot or save output for verification

---

## Manual Verification

- [ ] Follow `manual-test-script.md` (5 test scenarios)
- [ ] Test 1: OpenAI Responses works
- [ ] Test 2: OpenAI Chat works
- [ ] Test 3: Anthropic Messages works
- [ ] Test 4: Invalid combination handled
- [ ] Test 5: Missing key error is clear
- [ ] All manual tests ✓

---

## Code Review

### Stage 1: Traditional Review

- [ ] Run GPT-5-Codex /review with instructions from `prompts/CODEX-REVIEW.txt`
- [ ] Review focus:
  - [ ] Provider switching logic correct
  - [ ] Config handling safe (atomic writes, validation)
  - [ ] Error messages helpful and actionable
  - [ ] CLI UX clear (list-providers output, confirmation messages)
  - [ ] Factory pattern clean (no tight coupling)
- [ ] Address critical issues
- [ ] Document major/minor issues for Phase 7 or future work

### Stage 2: Port Validation Review

- [ ] Verify provider abstraction preserved (WireApi pattern maintained)
- [ ] Verify adapter patterns correct (clients normalize to ResponseItems)
- [ ] Review model integration results:
  - [ ] All providers work with real APIs
  - [ ] Config parameters work (thinking, temperature)
  - [ ] Any provider-specific quirks documented
- [ ] Address critical issues
- [ ] Document findings

---

## Documentation

- [ ] Update DECISIONS.md:
  - [ ] Default API per provider
  - [ ] Factory location (ConversationManager method vs separate module)
  - [ ] Client reuse strategy (fresh per conversation vs shared)
  - [ ] Config validation timing (save time vs creation time)
  - [ ] Model name validation approach (allow any vs validate against known)
  - [ ] Any provider-specific quirks discovered
- [ ] Update checklist (mark completed items)
- [ ] Verify phase ready for next phase

---

## Final Verification

- [ ] All tasks above completed
- [ ] All tests passing (mocked + integration)
- [ ] All quality gates passed
- [ ] Manual verification complete
- [ ] Code review complete (both stages)
- [ ] Documentation updated
- [ ] Ready to commit and move to Phase 4

---

**Phase 3 complete when all items checked ✓**


---

STANDARDS:

See docs/core/dev-standards.md for complete coding standards.
See docs/core/contract-testing-tdd-philosophy.md for testing approach.

Key requirements:
- TypeScript strict mode, no any types
- ESLint 0 errors
- Prettier formatted
- Mocked-service tests at library boundaries
- Mock all external dependencies


---

EXECUTION WORKFLOW:

EXECUTION WORKFLOW:

1. Read all reference documents (project context, phase design, standards, test conditions)
2. Review checklist (understand all tasks)
3. Write mocked-service tests FIRST (TDD):
   - Create test file based on test-conditions.md
   - Implement mocks (ModelClient, Config, etc.)
   - Write tests for each functional condition
   - Run tests (should fail - nothing implemented)
4. Implement code to pass tests:
   - Create files listed in checklist
   - Follow design.md implementation specifics
   - Run tests after each component
   - Iterate until all tests green
5. Manual functional testing:
   - Follow manual-test-script.md
   - Execute each test case
   - Verify expected behavior
   - Document any issues
6. Quality verification:
   - Run: npm run format (fix formatting)
   - Run: npm run lint (fix errors)
   - Run: npx tsc --noEmit (fix type errors)
   - Run: npm test (verify all pass)
   - Run combined: npm run format && npm run lint && npx tsc --noEmit && npm test
   - All must pass before proceeding
7. Update artifacts:
   - Update checklist.md (check off completed tasks)
   - Update decisions.md (log implementation choices with rationale)
8. Final verification:
   - All checklist items checked
   - All quality gates pass
   - Manual tests successful
   - Decisions documented
9. Commit and push
10. Report completion, ready for quality verifier and code reviewer

DO NOT declare phase complete until all steps verified.


---

MANUAL VERIFICATION:

# Phase 3: Manual Test Script

**Goal:** Verify CLI can switch among OpenAI Responses, OpenAI Chat, and Anthropic Messages APIs and that all three providers work correctly with real models.

**Prerequisites:**
- Phase 2 complete and merged (tool execution working)
- API keys configured in .env or ~/.cody/config.toml
- CLI built and available: `npm run build`

---

## Setup

**1. Verify API keys available:**
```bash
# Check keys are set
echo $OPENAI_API_KEY
echo $ANTHROPIC_API_KEY

# Or verify in config
cat ~/.cody/config.toml
```

**2. Set baseline config:**
```bash
cody set-provider openai --api responses --model gpt-4o-mini
```

**3. Verify baseline:**
```bash
cody list-providers
# Should show → openai/responses as current
```

---

## Test 1: OpenAI Responses API

**Objective:** Verify Responses API works end-to-end

**Steps:**
1. Create new conversation:
   ```bash
   cody new
   ```
   **Expected:** Prints "Created conversation: conv_XXXXX"

2. Send message:
   ```bash
   cody chat "Summarize what Cody CLI does in one sentence"
   ```
   **Expected:** 
   - Response from GPT-4o-mini displayed
   - Response is coherent summary
   - No errors

3. Multi-turn test:
   ```bash
   cody chat "What did I just ask you?"
   ```
   **Expected:**
   - Model remembers previous message
   - Response references the summary question

**Success criteria:**
- [ ] Conversation created successfully
- [ ] Responses received and displayed
- [ ] Multi-turn context maintained
- [ ] No errors or crashes

---

## Test 2: OpenAI Chat Completions API

**Objective:** Verify Chat API works and switching is seamless

**Steps:**
1. Switch to Chat API:
   ```bash
   cody set-api chat
   ```
   **Expected:** Prints "✓ API set to chat"

2. Verify switch persisted:
   ```bash
   cody list-providers
   ```
   **Expected:** Shows → openai/chat as current

3. Create new conversation:
   ```bash
   cody new
   ```

4. Send message:
   ```bash
   cody chat "Explain the difference between Responses and Chat APIs in one sentence"
   ```
   **Expected:**
   - Response from Chat API (gpt-4o-mini)
   - Response explains API difference
   - CLI shows provider: openai (chat)

5. Verify conversation works:
   ```bash
   cody chat "Give me an example use case for each"
   ```
   **Expected:** Multi-turn context maintained

**Success criteria:**
- [ ] API switch successful
- [ ] New conversation uses Chat API
- [ ] Responses coherent
- [ ] No errors

---

## Test 3: Anthropic Messages API

**Objective:** Verify Messages API works and provider switching works

**Steps:**
1. Switch to Anthropic:
   ```bash
   cody set-provider anthropic --api messages --model claude-3-haiku
   ```
   **Expected:** Prints "✓ Provider set to anthropic (messages)"

2. Verify switch:
   ```bash
   cody list-providers
   ```
   **Expected:** Shows → anthropic/messages as current

3. Create conversation:
   ```bash
   cody new
   ```

4. Send message:
   ```bash
   cody chat "List three tools available in Cody CLI"
   ```
   **Expected:**
   - Response from Claude (haiku-4.5)
   - May trigger tool calls (readFile to check available tools)
   - If tool approval requested → approve
   - Response lists tools (exec, readFile, applyPatch, etc.)

5. Verify Claude style:
   ```bash
   cody chat "What's your name?"
   ```
   **Expected:** Response identifies as Claude (confirms Messages API used)

**Success criteria:**
- [ ] Provider switch successful
- [ ] Messages API works
- [ ] Tool calls work with Anthropic (if triggered)
- [ ] Responses are Claude-style
- [ ] No errors

---

## Test 4: Invalid Combination Handling

**Objective:** Verify error handling for unsupported provider/API combinations

**Steps:**
1. Attempt invalid combo:
   ```bash
   cody set-provider anthropic --api chat
   ```
   **Expected:**
   - Error message: "Provider 'anthropic' does not support API 'chat'"
   - Lists supported APIs: "Supported APIs for anthropic: messages"
   - Exit code 1

2. Verify config unchanged:
   ```bash
   cody list-providers
   ```
   **Expected:** Still shows previous valid config (anthropic/messages from Test 3)

3. Attempt another invalid combo:
   ```bash
   cody set-provider openai --api messages
   ```
   **Expected:**
   - Error message: "Provider 'openai' does not support API 'messages'"
   - Lists supported: "Supported APIs for openai: responses, chat"

**Success criteria:**
- [ ] Invalid combinations rejected
- [ ] Error messages helpful and actionable
- [ ] Config remains in valid state
- [ ] No crashes

---

## Test 5: Missing API Key Error Handling

**Objective:** Verify clear error when API key missing

**Steps:**
1. Temporarily remove Anthropic key:
   ```bash
   # Backup current config
   cp ~/.cody/config.toml ~/.cody/config.toml.backup
   
   # Edit config to remove anthropic_api_key
   # Or unset env var if using that
   ```

2. Attempt to use Anthropic:
   ```bash
   cody set-provider anthropic --api messages
   cody new
   ```
   **Expected:**
   - Error: "Missing API key for provider anthropic"
   - Suggests: "Set in config: [auth]\nanthropic_api_key = \"...\""
   - Or: "Run: cody set-auth api-key"

3. Restore config:
   ```bash
   mv ~/.cody/config.toml.backup ~/.cody/config.toml
   ```

**Success criteria:**
- [ ] Missing key detected
- [ ] Error message clear and actionable
- [ ] Suggests how to fix
- [ ] No crash or hang

---

## Test 6: Provider Parity Check

**Objective:** Verify same conversation works across all providers

**Steps:**
1. Test with each provider using same prompt:
   ```bash
   # OpenAI Responses
   cody set-provider openai --api responses
   cody new
   cody chat "What is 2+2?"
   # Record response
   
   # OpenAI Chat
   cody set-api chat
   cody new
   cody chat "What is 2+2?"
   # Record response
   
   # Anthropic Messages
   cody set-provider anthropic --api messages
   cody new
   cody chat "What is 2+2?"
   # Record response
   ```

2. Compare responses:
   - All should answer "4" (or equivalent)
   - All should be coherent
   - Format may differ slightly (provider style) but content equivalent

**Success criteria:**
- [ ] All three providers respond correctly
- [ ] Responses are equivalent in content
- [ ] No provider fails or errors
- [ ] Switching between providers seamless

---

## Success Checklist

**Functional verification:**
- [ ] OpenAI Responses API works
- [ ] OpenAI Chat API works
- [ ] Anthropic Messages API works
- [ ] Provider switching persists
- [ ] list-providers shows current selection
- [ ] Invalid combinations rejected gracefully
- [ ] Missing key errors are clear and actionable
- [ ] Provider parity confirmed (same conversation works on all)

**Quality verification:**
- [ ] All mocked-service tests passing (11 tests)
- [ ] All model integration scripts passing (6 scripts)
- [ ] TypeScript: 0 errors
- [ ] ESLint: 0 errors
- [ ] Format: clean
- [ ] Combined: All checks pass

**Code review:**
- [ ] Stage 1 complete (traditional review)
- [ ] Stage 2 complete (port validation)
- [ ] Critical issues addressed

---

**All checks ✓ → Phase 3 functional verification complete**

**Next:** Commit changes, update project logs, proceed to Phase 4


---

FINAL QUALITY CHECK:

Before declaring phase complete:

Run: npm run format && npm run lint && npx tsc --noEmit && npm test

ALL must pass. Document results.
Update checklist.md and decisions.md.
Commit and push.
Ready for verification stages.

===== END CODER PROMPT =====
